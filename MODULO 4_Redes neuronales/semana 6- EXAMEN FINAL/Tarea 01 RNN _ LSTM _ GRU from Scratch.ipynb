{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 952,
     "status": "ok",
     "timestamp": 1594441298176,
     "user": {
      "displayName": "Marchelo Bragagnini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwqJ8TfMtaZO5tGKTw93K_q2UXX-xdE2JZXReM=s64",
      "userId": "12739426923871783271"
     },
     "user_tz": 300
    },
    "id": "xsQu1qomyBKp"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1VC3MuGc_D-d"
   },
   "source": [
    "# Vanilla RNN Forward & Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcDni2bW_LNf"
   },
   "source": [
    "## Paso Forward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 943,
     "status": "ok",
     "timestamp": 1594441298176,
     "user": {
      "displayName": "Marchelo Bragagnini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwqJ8TfMtaZO5tGKTw93K_q2UXX-xdE2JZXReM=s64",
      "userId": "12739426923871783271"
     },
     "user_tz": 300
    },
    "id": "ZI9fVF5B6m8u"
   },
   "outputs": [],
   "source": [
    "def rnn_one_step_forward(xt, h_prev, parameters):\n",
    "    Whx = parameters[\"Whx\"]\n",
    "    Whh = parameters[\"Whh\"]\n",
    "    Wyh = parameters[\"Wyh\"]\n",
    "    bh = parameters[\"bh\"]\n",
    "    by = parameters[\"by\"]  \n",
    "\n",
    "    # forward pass\n",
    "    h_next = np.tanh(np.dot(Whx, xt) + np.dot(Whh, h_prev) + bh)\n",
    "    yt_hat = softmax(np.dot(Wyh, h_next) + by)\n",
    "\n",
    "    cache = (h_next, h_prev, xt, parameters)\n",
    "\n",
    "    return h_next, yt_hat, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Innqk5U99sFY"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 934,
     "status": "ok",
     "timestamp": 1594441298177,
     "user": {
      "displayName": "Marchelo Bragagnini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwqJ8TfMtaZO5tGKTw93K_q2UXX-xdE2JZXReM=s64",
      "userId": "12739426923871783271"
     },
     "user_tz": 300
    },
    "id": "-mCX1ps5zR4z"
   },
   "outputs": [],
   "source": [
    "def rnn_forward(x, h0, parameters):\n",
    "  caches = []\n",
    "\n",
    "  n_x, m, T_x = x.shape\n",
    "  n_y, n_h = parameters[\"Wyh\"].shape\n",
    "\n",
    "  H = np.zeros((n_h, m, T_x))\n",
    "  Yt_hat = np.zeros((n_y, m, T_x))\n",
    "\n",
    "  h_next = h0\n",
    "\n",
    "  for t in range(T_x):\n",
    "    h_next, yt_hat, cache = rnn_one_step_forward(x[:,:,t], h_next, parameters)\n",
    "    H[:,:,t] = h_next\n",
    "    Yt_ha[:,:,t] = yt_hat\n",
    "    caches.append(cache)\n",
    "\n",
    "  caches = (caches, x)\n",
    "  return H, Yt_hat, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RRNpby6Q_OYz"
   },
   "source": [
    "## Paso Backward\n",
    "**Nota:** Se está considerando el BackPropagation Through Time (BPTT) truncado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 926,
     "status": "ok",
     "timestamp": 1594441298178,
     "user": {
      "displayName": "Marchelo Bragagnini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwqJ8TfMtaZO5tGKTw93K_q2UXX-xdE2JZXReM=s64",
      "userId": "12739426923871783271"
     },
     "user_tz": 300
    },
    "id": "c5sGRgbu6sns"
   },
   "outputs": [],
   "source": [
    "def rnn_one_step_backward(dh_next, cache):\n",
    "  (h_next, h_prev, xt, parameters) = cache\n",
    "  Whx = parameters[\"Whx\"]\n",
    "  Whh = parameters[\"Whh\"]\n",
    "  Wyh = parameters[\"Wyh\"]\n",
    "  bh = parameters[\"bh\"]\n",
    "  by = parameters[\"by\"]\n",
    "\n",
    "  # gradients  \n",
    "  dtanh = (1 - h_next ** 2) * dh_next \n",
    "  dWhx = np.dot(dtanh, xt.T)  \n",
    "  dWhh = np.dot(dtanh, h_prev.T)\n",
    "  dbh = np.sum(dtanh, axis = 1,keepdims=1)\n",
    "\n",
    "  # inputs \n",
    "  dxt = np.dot(Whx.T, dtanh)\n",
    "  dh_prev = np.dot(Whh.T, dtanh)\n",
    "\n",
    "  gradients = {\"dxt\": dxt, \"dh_prev\": dh_prev, \"dWhx\": dWhx, \"dWhh\": dWh, \"dba\": dbh}\n",
    "\n",
    "  return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 916,
     "status": "ok",
     "timestamp": 1594441298178,
     "user": {
      "displayName": "Marchelo Bragagnini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwqJ8TfMtaZO5tGKTw93K_q2UXX-xdE2JZXReM=s64",
      "userId": "12739426923871783271"
     },
     "user_tz": 300
    },
    "id": "kdMYNIIs8abV"
   },
   "outputs": [],
   "source": [
    "def rnn_backward(dh, caches):\n",
    "  (caches, x) = caches\n",
    "  (h1, h0, x1, parameters) = caches[0]\n",
    "\n",
    "  n_h, m, T_x = dh.shape\n",
    "  n_x, m = x1.shape\n",
    "\n",
    "  # inicialización de los gradientes\n",
    "  dx = np.zeros((n_x, m, T_x))\n",
    "  dWhx = np.zeros((n_h, n_x))\n",
    "  dWh = np.zeros((n_h, n_h))\n",
    "  dbh = np.zeros((n_h, 1))\n",
    "  dh0 = np.zeros((n_h, m))\n",
    "  dh_prevt = np.zeros((n_h, m))    \n",
    "\n",
    "  # Loop para cada time-step\n",
    "  for t in reversed(range(T_x)):\n",
    "    gradients = rnn_one_step_backward(dh[:,:,t] + dh_prevt, caches[t])\n",
    "\n",
    "    dxt, dh_prevt = gradients[\"dxt\"], gradients[\"dh_prev\"], \n",
    "    dWhxt, dWhht, dbht = gradients[\"dWhx\"], gradients[\"dWhh\"], gradients[\"dbh\"]\n",
    "    \n",
    "    dx[:, :, t] = dxt\n",
    "    dWhx += dWhxt\n",
    "    dWhh += dWhht\n",
    "    dbh += dbht\n",
    "      \n",
    "  # derivada caso base\n",
    "  da0 = dh_prevt\n",
    "  gradients = {\"dx\": dx, \"dh0\": dh0, \"dWhx\": dWhx, \"dWhh\": dWhh,\"dbh\": dbh}\n",
    "\n",
    "  return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQQUH0R5_S6U"
   },
   "source": [
    "# Long Short Term Memory (LSTM) Forward & Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qs0c8hclEkpl"
   },
   "source": [
    "La siguiente imagen muestra la arquitectura de una LSTM. \n",
    "\n",
    "![picture](https://drive.google.com/uc?id=11A9DOUP1F0ztCDn7-cy38S_Yc2GfHG7u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ass1y4lR_cnR"
   },
   "source": [
    "## Paso Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1594441298179,
     "user": {
      "displayName": "Marchelo Bragagnini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwqJ8TfMtaZO5tGKTw93K_q2UXX-xdE2JZXReM=s64",
      "userId": "12739426923871783271"
     },
     "user_tz": 300
    },
    "id": "OC53BZYh-8YU"
   },
   "outputs": [],
   "source": [
    "def lstm_one_step_forward(xt, h_prev, c_prev, parameters):\n",
    "  Wf = parameters[\"Wf\"]\n",
    "  bf = parameters[\"bf\"]\n",
    "  Wi = parameters[\"Wi\"]\n",
    "  bi = parameters[\"bi\"]\n",
    "  Wc = parameters[\"Wc\"]\n",
    "  bc = parameters[\"bc\"]\n",
    "  Wo = parameters[\"Wo\"]\n",
    "  bo = parameters[\"bo\"]\n",
    "  Wy = parameters[\"Wy\"]\n",
    "  by = parameters[\"by\"]\n",
    "\n",
    "  # dimensiones del xt y WY \n",
    "  n_x, m = xt.shape\n",
    "  n_y, n_h = Wy.shape\n",
    "\n",
    "  # concatenación de h_prev y xt\n",
    "  concat = np.zeros((n_h + n_x, m))\n",
    "  concat[: n_h, :] = h_prev\n",
    "  concat[n_h :, :] = xt\n",
    "\n",
    "  # compuerta de olvido\n",
    "  ft = sigmoid(np.dot(Wf, concat) + bf)\n",
    "  # compuerta de entrada\n",
    "  it = sigmoid(np.dot(Wi, concat) + bi)\n",
    "  # new cell memory\n",
    "  cct = np.tanh(np.dot(Wc, concat) + bc)\n",
    "  # la nueva memoria\n",
    "  c_next = ft * c_prev + it * cct\n",
    "  # compuerta de salida\n",
    "  ot = sigmoid(np.dot(Wo, concat) + bo)\n",
    "  # nuevo estado oculto\n",
    "  h_next = ot * np.tanh(c_next)\n",
    "\n",
    "  # el yhat predecido\n",
    "  yt_hat = softmax(np.dot(Wy, h_next) + by)\n",
    "  \n",
    "  cache = (h_next, c_next, h_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
    "\n",
    "  return h_next, c_next, yt_hat, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1179,
     "status": "ok",
     "timestamp": 1594441298463,
     "user": {
      "displayName": "Marchelo Bragagnini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwqJ8TfMtaZO5tGKTw93K_q2UXX-xdE2JZXReM=s64",
      "userId": "12739426923871783271"
     },
     "user_tz": 300
    },
    "id": "zZzTeoDrBP1t"
   },
   "outputs": [],
   "source": [
    "def lstm_forward(x, h0, parameters):\n",
    "\n",
    "  caches = []\n",
    "\n",
    "  n_x, m, T_x = x.shape\n",
    "  n_y, n_h = parameters[\"Wy\"].shape\n",
    "\n",
    "  H = np.zeros((n_h, m, T_x))\n",
    "  C = np.zeros((n_h, m, T_x))\n",
    "  Y = np.zeros((n_y, m, T_x))\n",
    "\n",
    "  h_next = h0\n",
    "  c_next = np.zeros(h_next.shape)\n",
    "\n",
    "  for t in range(T_x):\n",
    "    h_next, c_next, yt, cache = lstm_cell_forward(x[:, :, t], h_next, \n",
    "                                                  c_next, parameters)\n",
    "    \n",
    "    H[:,:,t] = h_next\n",
    "    Y[:,:,t] = yt\n",
    "    C[:,:,t]  = c_next\n",
    "    caches.append(cache)\n",
    "      \n",
    "  caches = (caches, x)\n",
    "\n",
    "  return H, Y, C, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pEylSkUrCR0u"
   },
   "source": [
    "## Paso Backward\n",
    "\n",
    "**Nota:** Se está considerando el BackPropagation Through Time (BPTT) truncado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1171,
     "status": "ok",
     "timestamp": 1594441298465,
     "user": {
      "displayName": "Marchelo Bragagnini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwqJ8TfMtaZO5tGKTw93K_q2UXX-xdE2JZXReM=s64",
      "userId": "12739426923871783271"
     },
     "user_tz": 300
    },
    "id": "g_q6-hgvCNbh"
   },
   "outputs": [],
   "source": [
    "def lstm_one_step_backward(dh_next, dc_next, cache):\n",
    "    \n",
    "    # extrayendo valores de cache\n",
    "    (h_next, c_next, h_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache\n",
    "        \n",
    "    n_x, m = xt.shape\n",
    "    n_h, m = h_next.shape\n",
    "\n",
    "    # derivadas de las compuerta \n",
    "    # dos_t: derivada de la compuerta de salida\n",
    "    # dccs_t: derivada de la new cell memory\n",
    "    # dis_t: derivada de la compuerta de entrada\n",
    "    # dfs_t: derivada de la compuerta de olvido\n",
    "\n",
    "    dos_t = dh_next * np.tanh(c_next) * ot * (1 - ot)\n",
    "    dccs_t = (dh_next * ot * (1 - np.tanh(c_next) ** 2) + dc_next) * it * (1 - cct ** 2)\n",
    "    dis_t = (dh_next * ot * (1 - np.tanh(c_next) ** 2) + dc_next) * cct * (1 - it) * it\n",
    "    dfs_t = (dh_next * ot * (1 - np.tanh(c_next) ** 2) + dc_next) * c_prev * ft * (1 - ft)\n",
    "    \n",
    "    # derivada de los pesos:\n",
    "    # dW{f,i,c,o}: matriz de las compuertas de olvido(f), entrada(i),\n",
    "    #             new cell memoria (c), salida (o)\n",
    "    # db{f,i,c,o}: bias de las compuertas de olvido(f), entrada(i),\n",
    "    #             new cell memoria (c), salida (o)\n",
    "    dWf = np.dot(dfs_t, np.hstack([h_prev.T, xt.T]))\n",
    "    dWi = np.dot(dis_t, np.hstack([h_prev.T, xt.T]))\n",
    "    dWc = np.dot(dccs_t, np.hstack([h_prev.T, xt.T]))\n",
    "    dWo = np.dot(dos_t, np.hstack([h_prev.T, xt.T]))\n",
    "    dbf = np.sum(dfs_t, axis=1, keepdims=True)\n",
    "    dbi = np.sum(dis_t, axis=1, keepdims=True)\n",
    "    dbc = np.sum(dccs_t, axis=1, keepdims=True)\n",
    "    dbo = np.sum(dos_t, axis=1, keepdims=True)\n",
    "\n",
    "    dh_prev = np.dot(Wf[:, :n_h].T, dfs_t) + np.dot(Wc[:, :n_h].T, dccs_t)\n",
    "    dh_prev += np.dot(Wi[:, :n_h].T, dis_t) + np.dot(Wo[:, :n_h].T, dos_t)\n",
    "\n",
    "    dc_prev = (dh_next * ot * (1 - np.tanh(c_next) ** 2) + dc_next) * ft\n",
    "    dxt = np.dot(Wf[:, n_h:].T, dfs_t) + np.dot(Wc[:, n_h:].T, dccs_t)\n",
    "    dxt += np.dot(Wi[:, n_h:].T, dis_t) + np.dot(Wo[:, n_h:].T, dos_t)\n",
    "    \n",
    "    gradients = {\"dxt\": dxt, \"dh_prev\": dh_prev, \"dc_prev\": dc_prev, \n",
    "                 \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                 \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1161,
     "status": "ok",
     "timestamp": 1594441298467,
     "user": {
      "displayName": "Marchelo Bragagnini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwqJ8TfMtaZO5tGKTw93K_q2UXX-xdE2JZXReM=s64",
      "userId": "12739426923871783271"
     },
     "user_tz": 300
    },
    "id": "I56gjavkEcbz"
   },
   "outputs": [],
   "source": [
    "def lstm_backward(dh, caches):\n",
    "\n",
    "  # extrayendo valores de la cache\n",
    "  (caches, x) = caches\n",
    "  (h1, c1, h0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]\n",
    "\n",
    "  n_h, m, T_x = dh.shape\n",
    "  n_x, m = x1.shape\n",
    "\n",
    "  # inicializando los parametros\n",
    "  dx = np.zeros((n_x, m, T_x))\n",
    "  dh0 = np.zeros((n_h, m))\n",
    "  dh_prevt = np.zeros((n_h, m))\n",
    "  dc_prevt = np.zeros((n_h, m))\n",
    "  dWf = np.zeros((n_h, n_h + n_x))\n",
    "  dWi = np.zeros((n_h, n_h + n_x))\n",
    "  dWc = np.zeros((n_h, n_h + n_x))\n",
    "  dWo = np.zeros((n_h, n_h + n_x))\n",
    "  dbf = np.zeros((n_h, 1))\n",
    "  dbi = np.zeros((n_h, 1))\n",
    "  dbc = np.zeros((n_h, 1))\n",
    "  dbo = np.zeros((n_h, 1))\n",
    "\n",
    "  # loop a travez del time-step\n",
    "  for t in reversed(range(T_x)):\n",
    "    gradients = lstm_cell_backward(dh[:,:,t] + dh_prevt, dc_prevt, caches[t])\n",
    "\n",
    "    dx[:,:,t] = gradients[\"dxt\"]\n",
    "    dWf += gradients[\"dWf\"]\n",
    "    dWi += gradients[\"dWi\"]\n",
    "    dWc += gradients[\"dWc\"]\n",
    "    dWo += gradients[\"dWo\"]\n",
    "    dbf += gradients[\"dbf\"]\n",
    "    dbi += gradients[\"dbi\"]\n",
    "    dbc += gradients[\"dbc\"]\n",
    "    dbo += gradients[\"dbo\"]\n",
    "\n",
    "  dh0 = gradients[\"dh_prev\"]\n",
    "      \n",
    "  gradients = {\"dx\": dx, \"dh0\": dh0, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "              \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "\n",
    "  return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7xUbWnu-7kWz"
   },
   "source": [
    "# Gated Recurrent Units (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hEYAkZPW8z1Z"
   },
   "source": [
    "En cada sección debes de codificar las funciones correspondientes al paso forward y backward, tal como se hizo para Vanilla RNN y LSTM. Se te dará un esqueleto base para codificar, si gusta modificalas a tu manera. En caso de encontrar un error, **esto de existir serán adrede ;)** , sientete libre de modificar las funciones. Considera que el y predecido corresponde a un problema de multi-clasificación \n",
    "\n",
    "Se muestra una imagen de la arquitectura de una GRU. Imagen credito de CS224n: Natural Language Processing with Deep Learning, Chris Manning.\n",
    "![picture](https://drive.google.com/uc?id=1A2i7TH_RXJITXNixkO1UUlibOXHYqw5f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qio86QHq7ox3"
   },
   "source": [
    "## Paso Forward (20 puntos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EXka6gxhDpYu"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 997,
     "status": "ok",
     "timestamp": 1594456085723,
     "user": {
      "displayName": "Marchelo Bragagnini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwqJ8TfMtaZO5tGKTw93K_q2UXX-xdE2JZXReM=s64",
      "userId": "12739426923871783271"
     },
     "user_tz": 300
    },
    "id": "EJZaWtmJ9wDF"
   },
   "outputs": [],
   "source": [
    "def gru_one_step_forward(xt, h_prev, parameters):\n",
    "  # parametros\n",
    "  # ==> CODIFICA AQUI\n",
    "\n",
    "  # dimensiones del xt y WY \n",
    "  # ==> CODIFICA AQUI\n",
    "\n",
    "  # concatenación de h_prev y xt\n",
    "  # ==> CODIFICA AQUI\n",
    "\n",
    "  # COMPUERTAS\n",
    "  # ==> CODIFICA AQUI\n",
    "\n",
    "  # el yhat predecido\n",
    "  # ==> CODIFICA AQUI\n",
    "  \n",
    "  # almacenamienot para el paso backward\n",
    "  # ==> codifica aqui\n",
    "\n",
    "  return h_next, c_next, yt_hat, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EW0m-Ah3-vmP"
   },
   "outputs": [],
   "source": [
    "def gru_forward(x, h0, parameters):\n",
    "\n",
    "  caches = []\n",
    "\n",
    "  n_x, m, T_x = x.shape\n",
    "  n_y, n_h = parameters[\"Wy\"].shape\n",
    "\n",
    "  H = np.zeros((n_h, m, T_x))\n",
    "  C = np.zeros((n_h, m, T_x))\n",
    "  Y = np.zeros((n_y, m, T_x))\n",
    "\n",
    "  h_next = h0\n",
    "  c_next = np.zeros(h_next.shape)\n",
    "\n",
    "  for t in range(T_x):\n",
    "    # ==> codifica aqui\n",
    "      \n",
    "  caches = (caches, x)\n",
    "\n",
    "  return H, Y, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rwHWq2y37yLt"
   },
   "source": [
    "## Paso Backward (30 puntos)\n",
    "\n",
    "**Nota:** Puedes usar BackPropagation Through Time (BPTT) truncado. Pero hay puntos extras si realizas el BPTT normal. Te recomiendo que antes de codificar hagas las derivaciones a mano, tal como se hizo en clase para vanilla RNN y LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 883,
     "status": "ok",
     "timestamp": 1594456624557,
     "user": {
      "displayName": "Marchelo Bragagnini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwqJ8TfMtaZO5tGKTw93K_q2UXX-xdE2JZXReM=s64",
      "userId": "12739426923871783271"
     },
     "user_tz": 300
    },
    "id": "BHZE-wrDAEZz"
   },
   "outputs": [],
   "source": [
    "def gru_one_step_backward(dh_next, cache):\n",
    "    \n",
    "    # extrayendo valores de cache\n",
    "    # guiate de la misma función del LSTM y RNN\n",
    "    # ===> CODEA AQUI\n",
    "        \n",
    "    n_x, m = xt.shape\n",
    "    n_h, m = h_next.shape\n",
    "\n",
    "    # derivadas de las compuerta \n",
    "    # considera cuantas compuertas tiene el GRU, y mira la diferencia con las\n",
    "    # LSTM\n",
    "    # ===> CODEA AQUI\n",
    "    \n",
    "    # derivada de los pesos:\n",
    "    # dW{f,i,c,o}: matriz de las compuertas de ___, ___, ___\n",
    "    # db{f,i,c,o}: bias de las compuertas de ___, ___, ___\n",
    "    # ===> CODEA AQUI\n",
    "\n",
    "    # derivada de h_prev, \n",
    "    # ===> CODEA AQUI\n",
    "\n",
    "    # derivada de dxt\n",
    "    # ===> CODEA AQUI\n",
    "    \n",
    "    # almanacena las derivadas calculadas en un diccionario \"gradients\"\n",
    "    # ===> CODEA AQUI\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 998,
     "status": "ok",
     "timestamp": 1594456732200,
     "user": {
      "displayName": "Marchelo Bragagnini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwqJ8TfMtaZO5tGKTw93K_q2UXX-xdE2JZXReM=s64",
      "userId": "12739426923871783271"
     },
     "user_tz": 300
    },
    "id": "PsQ-b7mMBUmY"
   },
   "outputs": [],
   "source": [
    "def gru_backward(dh, caches):\n",
    "\n",
    "  # extrayendo valores de la cache\n",
    "  (caches, x) = caches\n",
    "  # ===> CODEA AQUI\n",
    "\n",
    "  n_h, m, T_x = dh.shape\n",
    "  n_x, m = x1.shape\n",
    "\n",
    "  # inicializando los parametros\n",
    "  # ===> CODEA AQUI\n",
    "\n",
    "  # loop a travez del time-step\n",
    "  for t in reversed(range(T_x)):\n",
    "    # ===> CODEA AQUI\n",
    "\n",
    "  dh0 = gradients[\"dh_prev\"]\n",
    "      \n",
    "  # almanacena las derivadas calculadas en un diccionario \"gradients\"\n",
    "  # ===> CODEA AQUI     \n",
    "\n",
    "  return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DKsMNfcq723f"
   },
   "source": [
    "## Derivaciones (50 puntos)\n",
    "\n",
    "Realiza las derivaciones a mano para el BPTT para la GRU, tal como se hizo en clase. Se muestra un ejemplo parcial para LSTM. Recuerda no solo expresar cada derivada usando la regla de la cadena, si no, también realiza la derivada de la función tanh, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WKnUKaVBDaW4"
   },
   "source": [
    "![picture](https://drive.google.com/uc?id=1YdZsdX6TGjsFqy96WqIrh47_QZPrLu3g)\n",
    "\n",
    "Este ejemplo muestra las derivadas de las compuertas, pero no de la matriz de peso y el bias. Tu tienes que hallar de todo.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOljb7bfho3By2WA+jeWY5W",
   "collapsed_sections": [],
   "name": "Tarea 01 RNN & LSTM & GRU from Scratch.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
