{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJ7Byh_K7POY"
   },
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "**Nombres y Apellidos:** *Escriba aqui*\n",
    "\n",
    "En este taller implementaremos el algoritmo de PCA desde cero. Los principales tópicos que revisaremos son:\n",
    "\n",
    "- Implementación del algoritmo de PCA.\n",
    "- Reducción de dimensionalidad de un conjunto de datos 2D a 1D.\n",
    "- Aplicación sobre el conjunto de datos de flores de iris.\n",
    "- Selección del número de componentes principales.\n",
    "- Uso de implementación de la librería scikit-learn.\n",
    "- Aplicación sobre un conjunto de datos tabular.\n",
    "\n",
    "Empecemos importando las librerías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nHctoTZS6YG2"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KgBN3z-UCf8o"
   },
   "source": [
    "## Implementación de PCA\n",
    "\n",
    "A continuación realizaremos la implementación de PCA, para ello es necesario seguir los siguientes pasos:\n",
    "\n",
    "1. Estandarizar el data (standardize).\n",
    "2. Calcular la matriz de covarianza para las características.\n",
    "3. Calcular los autovalores y autovectores de la matriz de covarianza.\n",
    "4. Ordenar los autovalores y sus correspondientes autovectores.\n",
    "5. Seleccionar 'k' autovectores\n",
    "6. Transformar la data original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBqv1pa1ibuv"
   },
   "source": [
    "### Data de Prueba\n",
    "\n",
    "Para probar nuestra implementación, haremos uso de datos generados simples para poder visualizarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAqXO7HviqFn"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Especificamos un random state para reproducir la generación de la data\n",
    "rng = np.random.RandomState(1)\n",
    "# Esta es la data con que se deberá probar la implementación\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "# Visualizamos la data\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C97cvHS59bz2"
   },
   "source": [
    "### Estandarizar la data\n",
    "\n",
    "Estandarizamos la data para que la distribución de cada característica tenga una media igual a cero y una desviación estándar igual uno (varianza unitaria).\n",
    "\n",
    "$$X_{new} = \\frac{X - \\mu}{ \\sigma }$$\n",
    "\n",
    "Es importante recalcar que algunas implementaciones y librerías no escalan sus datos para tener una varianza unitaria, por lo tanto, solamente realizan el centrado de datos ($X - \\mu$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kY7P1ikkA0Ia"
   },
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    \"\"\"Estandarizar el dataset X -> (X - mean) / std\n",
    "    Args:\n",
    "        X: dataset de dimensión (N x D), donde D es la dimensión de la data,\n",
    "            y N el número de puntos.\n",
    "    \n",
    "    Returns:\n",
    "        (X_norm, mu, std): X_norm es la data estandarizada con media 0\n",
    "        y desviación estándar 1; mu y std son la media y desviación estándar\n",
    "        respectivamente.\n",
    "    \n",
    "    Note:\n",
    "        Es probable que encuentres dimensiones donde la desviación estándar es 0,\n",
    "        y ello implica una división por cero (NaN). Para manejar ese escenario,\n",
    "        cuando reemplaza la desviación estándar de 0 por 1 antes de estandarizar.\n",
    "    \"\"\"\n",
    "    # Calcular la media y desviación estandar\n",
    "    mu = np.mean(X,axis=0)\n",
    "    std = np.std(X,axis=0)\n",
    "    print(\"stdddd: \",std)\n",
    "    # Reemplazar los valores de std=0 con 1\n",
    "    std_filled= std.copy()\n",
    "    std_filled[std==0] = 1\n",
    "    print(\"std_ fileld: \",std_filled)\n",
    "    # Estandarizar la data\n",
    "    X_norm  =(X-mu)/std\n",
    "    return X_norm, mu, std_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OfFOEkksivHZ"
   },
   "source": [
    "Estandarizamos la data de prueba y obtenemos la data estandarizada, la media y desviación estándar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XHEQkJk4iyN4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdddd:  [0.823873   0.31358832]\n",
      "std_ fileld:  [0.823873   0.31358832]\n",
      "Media [ 0.03351168 -0.00408072]\n",
      "Desviación Estándar [0.823873   0.31358832]\n",
      "Media de la nueva data:  [-1.77635684e-17 -4.44089210e-18]\n",
      "Desviación Estándar de la  nueva data:  [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "X_norm, mu, std = standardize(X)\n",
    "\n",
    "print(\"Media\", mu)\n",
    "print(\"Desviación Estándar\", std)\n",
    "\n",
    "print(\"Media de la nueva data: \", X_norm.mean(0))\n",
    "print(\"Desviación Estándar de la  nueva data: \", X_norm.std(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7e4d_Txi_rK"
   },
   "source": [
    "Algo importante a mencionar es que cuando tenemos dos conjuntos de datos, por ejemplo, entrenamiento y test. Hallamos la média y desviación estándar de la data de entrenamiento y usamos esa média y desviación estándar para estandarizar la data de test. Un proceso similar veremos cuando reduzcamos la dimensionalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYa7sG40Npi1"
   },
   "source": [
    "### Calcular la matriz de covarianza para las características\n",
    "\n",
    "La matriz de covarianza para un dataset de dimensión $(N, D)$ está dada por:\n",
    "\n",
    "$$ Cov(X) = \\frac{1}{N - 1}(X - \\bar{X})^T(X - \\bar{X}) $$\n",
    "\n",
    "donde $\\bar{X}$ es la media de cada característica.\n",
    "En el paso anterior hemos escalado los datos para que tengan media 0, por tanto la matriz de covarianza se reduce a lo siguiente:\n",
    "\n",
    "$$ Cov(X) = \\frac{1}{N - 1}X^{T}X$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i9t6Gk3_OyDm"
   },
   "outputs": [],
   "source": [
    "def covariance_matrix(X):\n",
    "    \"\"\"Calculamos la matriz de covarianza de X\n",
    "      Args:\n",
    "          X: dataset de dimensión (N x D), donde D es la dimensión de la data,\n",
    "             y N el número de puntos.\n",
    "      \n",
    "      Returns:\n",
    "          cov: matriz de covarianza de dimensión (D x D).\n",
    "    \"\"\"\n",
    "    cov = np.cov(X)\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lWm_z8gxkQ0I"
   },
   "source": [
    "Mostramos los resultados de la matriz de covarianza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mx6-AdSHkPi9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de covarianza\n",
      "[[ 0.03654507  0.10434726 -0.06807994 ... -0.03853645  0.01880842\n",
      "  -0.04815528]\n",
      " [ 0.10434726  0.29794303 -0.19438885 ... -0.11003324  0.05370373\n",
      "  -0.13749792]\n",
      " [-0.06807994 -0.19438885  0.12682635 ...  0.07178968 -0.03503827\n",
      "   0.08970864]\n",
      " ...\n",
      " [-0.03853645 -0.11003324  0.07178968 ...  0.04063634 -0.01983331\n",
      "   0.05077931]\n",
      " [ 0.01880842  0.05370373 -0.03503827 ... -0.01983331  0.00968001\n",
      "  -0.02478377]\n",
      " [-0.04815528 -0.13749792  0.08970864 ...  0.05077931 -0.02478377\n",
      "   0.063454  ]]\n",
      "\n",
      "Matriz de covarianza - numpy\n",
      "[[1.00502513 0.89385925]\n",
      " [0.89385925 1.00502513]]\n"
     ]
    }
   ],
   "source": [
    "S = covariance_matrix(X_norm)\n",
    "# Mostramos la matriz de covarianza\n",
    "print(\"Matriz de covarianza\")\n",
    "print(S)\n",
    "\n",
    "# Mostramos los resultados a partir de la libreria de numpy\n",
    "print(\"\\nMatriz de covarianza - numpy\")\n",
    "# numpy recibe una matriz de (D, N) dimensiones por tanto\n",
    "# la convarianza se calcula cov = XX^T\n",
    "print(np.cov(X_norm.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRGmP6NrTS-d"
   },
   "source": [
    "### Autovalores y autovectores de la matriz de covarianza\n",
    "\n",
    "Recordemos que los autovectores representan las componentes principales que generan el subespacio sobre el cual proyectaremos nuestra data y los autovalores la varianza explicada o retenida por cada componente. Matemáticamente es posible calcular los autovalores y autovectores resolviendo el siguiente sistema de ecuaciones:\n",
    "\n",
    "$$Ab = \\lambda b$$\n",
    "\n",
    "donde $A$ es la matriz sobre la cual deseamos obtener los autovalores y autovectores, $b$ son los autovectores (eigenvectors) y $\\lambda$ son los autovalores (eigenvalues). En nuestro caso, calcularemos los autovectores y autovalores de la matriz de covarianza, es decir, la matriz $A$ representa la matriz de covarianza. Para facilitar el proceso, haremos uso de una implementación de la librería de algebra lineal, [linalg](https://numpy.org/doc/1.18/reference/generated/numpy.linalg.eig.html#numpy.linalg.eig), que permite calcular los autovalores y autovectores de una determinada matriz cuadrada, como la matriz de covarianza es cuadrada (D x D), es posible usar esa implementación.\n",
    "\n",
    "En esta parte haremos los pasos $3$ y $4$ al mismo tiempo: \n",
    "\n",
    "3. Calcular los autovalores y autovectores de la matriz de covarianza.\n",
    "4. Ordenar los autovalores y sus respectivos autovectores.\n",
    "\n",
    "Tomar en consideranción que la líbreria retorna un vector con los autovalores y una matriz con los autovectores, donde cada columna es un autovalor y autovector respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBdVbhRNeq6C"
   },
   "outputs": [],
   "source": [
    "def eig(S):\n",
    "    \"\"\"Calcular los autovalores y sus correspondientes autovectores de la \n",
    "       matriz de covarianza S.\n",
    "    Args:\n",
    "        S: matriz de covarianza de dimensión (D, D)\n",
    "    \n",
    "    Returns:\n",
    "        (eigvals, eigvecs): los autovalores (1, D) y autovectores (D, D)\n",
    "\n",
    "    Note:\n",
    "        los autovalores y autovectores deben ser ordenados en orden descendente\n",
    "        con respecto a los autovalores\n",
    "    \"\"\"\n",
    "    # Calcular los autovalores y autovectores la matriz de covarianza S\n",
    "    # usar el método np.linalg.eig para facilitar el proceso \n",
    "    eigvals, eigvecs =np.linalg.eig(S)\n",
    "\n",
    "    # Ordenar los autovalores y sus correspondientes autovectores \n",
    "    # Puede hacer uso de np.sort o np.argsort para ordenar los vectores\n",
    "    eigvals=np.sort(eigvals)\n",
    "    eigvecs =np.sort( eigvecs)\n",
    "    # * Tomar en cuenta que la primera columna de la matriz de autovalores,\n",
    "    #   corresponde con la primera columna de la matriz de autovectores\n",
    "    \n",
    "    return (eigvals, eigvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bsZoVt8ykhcp"
   },
   "source": [
    "Mostramos los autovalores y autovectores respectivos. Los autovalores deben estar ordenados de mayor a menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JnLpak3rkrRa"
   },
   "outputs": [],
   "source": [
    "eigvals, eigvecs = eig(S)\n",
    "\n",
    "for i in range(len(eigvals)):\n",
    "    print(\"Autovalor %d: %lf\" % (i+1, eigvals[i]))\n",
    "    print(\"Autovector %d: \" % (i+1))\n",
    "    print(eigvecs[:,i].reshape(1,len(eigvals)).T)\n",
    "    print(40 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UoDPmyjYm2H0"
   },
   "source": [
    "Ahora visualizemos los autovectores que representan las componentes sobre las cuales proyectaremos nuestra data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yAVeZsvtnwah"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,) (200,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-25924494585c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# solo para efectos de visualizar, aumentamos la magnitud de los autovectores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mvector\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0meigvecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meigvals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mdraw_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu_center\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu_center\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'equal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,) (200,) "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0, color='r')\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X_norm[:, 0], X_norm[:, 1], alpha=0.2)\n",
    "mu_center = X_norm.mean(0)\n",
    "for i in range(len(eigvals)):\n",
    "    # solo para efectos de visualizar, aumentamos la magnitud de los autovectores\n",
    "    vector  = eigvecs[:,i] * np.sqrt(eigvals[i])*2\n",
    "    draw_vector(mu_center, mu_center + vector)\n",
    "plt.axis('equal');    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "amrlWa-jtFMo"
   },
   "source": [
    "La longitud de cada autovector (autovalor) es un indicador de importancia de cada autovector en el nuevo eje de coordenadas. La primera componente siempre representará la mayor varianza proyectada de los datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7TtG6xXt7mO"
   },
   "source": [
    "### Proyección usando todas las componentes\n",
    "\n",
    "Podemos realizar la proyección usando el nuevo eje de coordenadas obtenido a partir de los autovectores, es decir, usaremos todos los autovectores para la proyección en el subespacio (nuevo eje de coordenadas). Más adelante realizaremos la proyección sobre un menor subespacio. Para ello, la proyección de nuestra data sobre los vectores que generan el nuevo subespacio (autovectores) esta data por:\n",
    "\n",
    "$$T = XB$$\n",
    "\n",
    "donde $B$ es la matriz columna de autovectores donde proyectaremos nuestra data y $T$ es la transformación de nuestra data original de un espacio vectorial origen a un subespacio vectorial destino dado por los autovectores. En otras palabras, $T$ es la proyección de nuestra data sobre las componentes principales (nuevo eje de coordenadas).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9huST9nvvoNL"
   },
   "outputs": [],
   "source": [
    "def transform_full(X, B):\n",
    "    \"\"\"Transformar la data X al nuevo eje de coordenadas generado por B.\n",
    "    Args:\n",
    "        X: dataset (N, D)\n",
    "        B: matriz columna de autovectores (D, D)\n",
    "\n",
    "    Returns:\n",
    "        T: dataset transformado sobre el nuevo eje de coordenadas (N, D)\n",
    "    \"\"\"  \n",
    "    T=X.dot(B)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JL6U7ZjHyVxt"
   },
   "source": [
    "Visualizamos la data en el nuevo eje de coordenadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IrEDqu1jyaeB"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (200,2) and (200,200) not aligned: 2 (dim 1) != 200 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-290bc1e93412>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_full\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meigvecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdraw_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-75-35f6d2f8ab11>\u001b[0m in \u001b[0;36mtransform_full\u001b[1;34m(X, B)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0mtransformado\u001b[0m \u001b[0msobre\u001b[0m \u001b[0mel\u001b[0m \u001b[0mnuevo\u001b[0m \u001b[0meje\u001b[0m \u001b[0mde\u001b[0m \u001b[0mcoordenadas\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \"\"\"  \n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mT\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (200,2) and (200,200) not aligned: 2 (dim 1) != 200 (dim 0)"
     ]
    }
   ],
   "source": [
    "T = transform_full(X_norm, eigvecs)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0, color='r')\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "def pca_2d_plot(T, title='Principal Components'):\n",
    "    plt.scatter(T[:, 0], T[:, 1], alpha=0.2)\n",
    "    draw_vector([0, 0], [0, 1])\n",
    "    draw_vector([0, 0], [1, 0])\n",
    "    plt.axis('equal');\n",
    "    plt.gca().set(xlabel='component 1', ylabel='component 2',\n",
    "            title=title,\n",
    "            xlim=(-4, 4), ylim=(-2, 2))\n",
    "\n",
    "pca_2d_plot(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7AzZmMJE1RtZ"
   },
   "source": [
    "Como podemos observar, el eje de coordenadas ha sido rotado en base a los autovectores y la data ha sido proyectada sobre el nuevo eje de coordenadas. A continuación haremos la proyección usando solo algunas componentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pPLglrx11qIy"
   },
   "source": [
    "### Reducción de Dimensionalidad\n",
    "\n",
    "En la sección anterior vimos la transformación de la data de un espacio vectorial inicial, a un subespacio generado por **todos** los autovectores también llamados componentes principales. Ahora realizaremos el mismo proceso pero considerando solamente **algunos** autovectores. Como los autovectores están ordenados con respecto a sus autovalores, las primeras componentes capturarán la mayor varianza de la data, por tanto cuando reduzcamos la dimensionalidad de $D$ a $K$, siempre tomaremos en consideramos los primeros $K$ autovectores.\n",
    "\n",
    "$$T = X\\hat{B}$$\n",
    "\n",
    "donde $T$ es la transformación del espacio vectorial de nuestra data a un subespacio generado solamente por los $K$ primeros autovectores y $\\hat{B}$ es una matriz de dimensionalidad (D, K), es decir, una submatriz de $B$ que considera solamente las primeras $K$ columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LjjxWsJV3oHy"
   },
   "outputs": [],
   "source": [
    "def transform_data(X, B, K):\n",
    "    \"\"\"Transformar la data X al nuevo eje de coordenadas generado por B\n",
    "       considerando las primeras K columnas de B.\n",
    "    Args:\n",
    "        X: dataset (N, D)\n",
    "        B: matriz columna de autovectores (D, D)\n",
    "        K: número de componentes a considerar en la transformación\n",
    "\n",
    "    Returns:\n",
    "        T: dataset transformado sobre el nuevo eje de coordenadas (N, K)\n",
    "        B_k: matriz columna de los primeros K autovectores (D, K)\n",
    "    \"\"\"  \n",
    "    B_k = \n",
    "    T = \n",
    "    return T, B_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ImKuGqGf5HCY"
   },
   "source": [
    "Es importante mencionar que en la función anterior estamos retornando la data con dimensionalidad reducida y los primeros $K$ autovectores que generan el subespacio reducido. Al igual que cuando escalamos nuestros datos (standardize), donde calculamos la media y desviación estándar solo sobre la data de entrenamiento y las usamos en la data de test, la matriz de autovectores se calcula solamente sobre la data de entrenamiento y se usa esa matriz para reducir la dimensionalidad de la data de test.\n",
    "\n",
    "\n",
    "Mostremos los resultados de la transformación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ozwBUr0v5ihg"
   },
   "outputs": [],
   "source": [
    "X_pca, B_k = transform_data(X_norm, eigvecs, 1)\n",
    "\n",
    "print(\"Dimensión de data original: \", X_norm.shape)\n",
    "print(\"Dimensión de data transformada: \", X_pca.shape)\n",
    "\n",
    "print(\"\\n Dimensión de la matriz de transformación: \", B_k.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7OJ7Exle6N7F"
   },
   "source": [
    "Visualizamos la data transformada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46bvYxh66TaW"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "def pca_1d_plot(T, title='Principal Components'):\n",
    "  # plot data\n",
    "  plt.scatter(T[:, 0], np.zeros(T.shape[0]))\n",
    "  plt.axis('equal');\n",
    "  plt.gca().set(xlabel='component 1',\n",
    "          title=title,\n",
    "          xlim=(-4, 4), ylim=(-3, 3))\n",
    "pca_1d_plot(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56PVAMii8Oke"
   },
   "source": [
    "Como podemos observar hemos transformado nuestra data de 2 dimensiones a 1 sola dimensión. La dimensión elegida es la que retiene la mayor varianza de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "205mxthN8bMn"
   },
   "source": [
    "### Reconstrucción de la data\n",
    "\n",
    "En la sección anterior hemos reducido la dimensionalidad de la data a una dimensión. Para entender mejor el efecto de esta reducción, podemos realizar la reconstrucción de la data mediante la transformación inversa de la data reducida, es decir, hallar la inversa de la matriz de autovectores y usarla para volver al eje de coordenadas inicial. Entonces tendremos lo siguiente:\n",
    "\n",
    "$$P = X\\hat{B}{\\hat{B}}^{-1} = T\\hat{B}^{-1}$$\n",
    "\n",
    "donde $T$ es la transformación que usamos para reducir la dimensionalidad, $\\hat{B}$ es la matriz conteniendo las primeras $K$ componentes y $\\hat{B}^{-1}$ es la matriz inversa de $\\hat{B}$. Debido a que $B$ es una matriz ortogonal, se cumple que:\n",
    "\n",
    "$$ \\hat{B}^{-1} = \\hat{B}^T$$\n",
    "\n",
    "por lo tanto la reconstrucción de la data sobre el eje de coordenadas inicial esta dado por:\n",
    "\n",
    "$$P = X\\hat{B}{\\hat{B}}^{T} = T\\hat{B}^{T}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E912KIlbAUKi"
   },
   "outputs": [],
   "source": [
    "def inverse_transform(T, B_k):\n",
    "    \"\"\"Reconstruir la data inicial a partir de la data transformada,\n",
    "       aplicando la transformación inversa de las componentes.\n",
    "\n",
    "    Args:\n",
    "        T: dataset transformado sobre el nuevo eje de coordenadas (N, K)\n",
    "        B_k: matriz columna de los primeros K autovectores (D, K)\n",
    "\n",
    "    Returns:\n",
    "        X_rec: dataset reconstruído (N, D)\n",
    "    \"\"\"      \n",
    "    X_rec = \n",
    "    return X_rec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkC0vfqJCRht"
   },
   "source": [
    "Mostremos los resultados de la transformación inversa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d8KepJBRCVVq"
   },
   "outputs": [],
   "source": [
    "X_rec = inverse_transform(X_pca, B_k)\n",
    "\n",
    "print(\"Dimensión de data reducida: \", X_pca.shape)\n",
    "print(\"Dimensión de data recuperada: \", X_rec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2kpWkfwDdh7"
   },
   "source": [
    "Visualicemos la data reconstruida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dKfMhHoCDion"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(X_norm[:, 0], X_norm[:, 1], alpha=0.3)\n",
    "plt.scatter(X_rec[:, 0], X_rec[:, 1], alpha=0.8, color='blue')\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cT0V-9vHD19J"
   },
   "source": [
    "Los puntos claros son los datos originales, mientras que los puntos oscuros son lo datos reconstruidos o proyectados. Esto muestra de manera más clara lo que significa una reducción de dimensionalidad de PCA, ya que se elimina la información a lo largo del eje o ejes principales menos importantes, dejando solo el o los componente(s) de los datos con la mayor varianza. La fracción de varianza que se corta (proporcional a la extensión de puntos sobre la línea formada en esta figura) es aproximadamente una medida de cuánta \"información\" se descarta en esta reducción de dimensionalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UBh5iV6vHxxD"
   },
   "source": [
    "### Método Genérico para PCA\n",
    "\n",
    "En las secciones anteriores hemos implementado PCA paso a paso. Ahora encapsularemos todo ese proceso en un solo método que recibirá como entrada la data estandarizada y el número de componentes, y retornará la data en una dimensionalidad reducida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Qe1OsT0KFDs"
   },
   "outputs": [],
   "source": [
    "def PCA(X, K):\n",
    "    \"\"\"Reducción de dimensionalidad de un conjunto de datos X usando K \n",
    "       componentes\n",
    "\n",
    "    Args:\n",
    "        X: dataset (N, D), se asume que la data ya está estandarizada\n",
    "        K: número de componentes a considerar en la transformación\n",
    "\n",
    "    Returns:\n",
    "        T: dataset transformado sobre el nuevo eje de coordenadas (N, K)\n",
    "        B_k: matriz columna de los primeros K autovectores (D, K)\n",
    "    \"\"\" \n",
    "    # 1. Calcular la matriz de covarianza para las características\n",
    "    \n",
    "\n",
    "    # 2. Calcular los autovalores y autovectores de la matriz de covarianza\n",
    "\n",
    "\n",
    "    # 3. Seleccionar 'K' autovectores y transformar la data original\n",
    "    T, B_k = \n",
    "\n",
    "    return T, B_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-l2RT1lHhgF"
   },
   "source": [
    "Probemos la implementación mediante visualizaciones para $K = 1$ y $K = 2$. Las visualizaciones deben de ser las mismas que las mostradas en pasos anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q_jT8u2KNQKG"
   },
   "outputs": [],
   "source": [
    "# Obtenemos data reducida 2D\n",
    "T2, _= PCA(X_norm, 2)\n",
    "\n",
    "# Obtenemos data reducida 1D\n",
    "T1, _= PCA(X_norm, 1)\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "pca_2d_plot(T2, \"Principal Components 2D\")\n",
    "plt.subplot(1, 2, 2)\n",
    "pca_1d_plot(T1, \"Principal Components 1D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QrpTvr5R38qG"
   },
   "source": [
    "### PCA usando Singular Value Decomponsition (SVD)\n",
    "\n",
    "Anteriormente hemos implementado PCA haciendo uso de descomposión espectral (Eigendecomposition). En este parte del taller, haremos uso de SVD, el cual es una forma más simple y eficiente de implementar debido a que podemos obtener directamente los autovectores y autovalores de la matriz de covarianza, además de tener los autovalores ya ordenados de mayor a menor. SVD descompone una matriz de cualquier dimensión (N, D) en tres matrices:\n",
    "\n",
    "$$ X = U S V^T$$\n",
    "\n",
    "donde $U$ es una matriz ortogonal que contiene los autovectores de la matriz $XX^T$, $S$ es una matriz diagonal que contiene los autovalores de la matriz $X^TX$ y $V$ es una matriz ortogonal que contiene los autovectores de la matriz $X^TX$. Como en nuestro problema estamos interesados en hallar los autovectores y autovalores de la matriz de covarianza ($X^TX$), entonces podemos usar SVD para obtenerlos directamente. Los pasos para reducir la dimensionalidad haciendo uso de SVD son:\n",
    "\n",
    "1. Estandarizar o centrar la data.\n",
    "2. Descomponer la matriz que representa la data ($X$) haciendo uso de SVD.\n",
    "3. Como la matriz $V$ contiene los autovectores de la matriz de covarianza, hallamos la transpuesta de la matriz que retorna SVD. Tener en cuenta que la última matriz de SVD es $V^T$ pero nosotros necesitamos $V$.\n",
    "4. Aplicar la transformación de $X$ haciendo uso de las primeras $K$ columnas de $V$ ($T$ = $X\\hat{V}$) de la misma forma que hicimos en secciones anteriores.\n",
    "\n",
    "Para descomponer la matriz haremos uso de la librería de [linalg](https://numpy.org/doc/1.18/reference/generated/numpy.linalg.svd.html) que nos retorna las $3$ matrices antes mencionadas. Tomar en consideranción que la líbreria retorna $S$ como un vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NVwfiuBv3-jD"
   },
   "outputs": [],
   "source": [
    "def PCA_SVD(X, K):\n",
    "    \"\"\"Reducción de dimensionalidad haciendo uso de SVD sobre un \n",
    "       conjunto de datos X usando K componentes\n",
    "\n",
    "    Args:\n",
    "        X: dataset (N, D), se asume que la data ya está estandarizada\n",
    "        K: número de componentes a considerar en la transformación\n",
    "\n",
    "    Returns:\n",
    "        T: dataset transformado sobre el nuevo eje de coordenadas (N, K)\n",
    "        V_k: matriz columna de los primeros K autovectores (D, K)\n",
    "    \"\"\" \n",
    "    # 1. Descomponer la matriz X haciendo uso de np.linalg.svd\n",
    "    U, S, Vt = \n",
    "    \n",
    "    # 2. Obtener la matriz 'V' que contiene los autovectores de la matriz de covarianza\n",
    "    V = \n",
    "\n",
    "    # 3. Seleccionar K autovectores y transformar la data original\n",
    "    T, V_k = \n",
    "\n",
    "    return T, V_k  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDOHWGfd4HH3"
   },
   "source": [
    "Probemos la implementación mediante visualizaciones para $K=1$ y $K=2$. Las visualizaciones deben de ser las mismas que las mostradas en pasos anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Pg9CkII4JxR"
   },
   "outputs": [],
   "source": [
    "# Obtenemos data reducida 2D\n",
    "T2, _= PCA_SVD(X_norm, 2)\n",
    "\n",
    "# Obtenemos data reducida 1D\n",
    "T1, _= PCA_SVD(X_norm, 1)\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "pca_2d_plot(T2, \"Principal Components 2D\")\n",
    "plt.subplot(1, 2, 2)\n",
    "pca_1d_plot(T1, \"Principal Components 1D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fpPiHZli4NqB"
   },
   "source": [
    "### Reducir la dimensionalidad a partir de las matrices $U$ y $S$ (opcional)\n",
    "\n",
    "Es posible reducir la dimensionalidad haciendo uso de las matrices $U$ y $S$ de SVD. Para ello, usaremos propiedades de matrices ortogonales. Para empezar, nosotros sabemos que la matriz $V$ contiene los autovectores de la matriz de covarianza $X^TX$ y para transformar nuestros datos al nuevo subespacio formado por los autovectores (reducir dimensionalidad), simplemente tenemos que usar la matriz $V$, es decir, $$T = XV$$\n",
    "\n",
    "Además, tenemos que la descomposición en valores singulares (SVD) esta dada por:\n",
    "\n",
    "$$X = USV^T$$\n",
    "\n",
    "combinamos ambas formulaciones, multiplicando $V$ en SVD:\n",
    "\n",
    "$$XV = USV^TV$$ \n",
    "\n",
    "debido a que $V$ es una matriz ortogonal, tenemos que: $V^TV = I$, donde $I$ es la matriz identidad. Por lo tanto:\n",
    "\n",
    "$$XV = US$$\n",
    "\n",
    "ya que $XV$ es nuestra transformación al nuevo subespacio donde podemos reducir la dimensionalidad, podemos concluir que para reducir la dimensionalidad en base a $U$ y $S$ basta con multiplicarlos.\n",
    "\n",
    "$$T = US$$\n",
    "\n",
    "Esta forma de reducir la dimensionalidad es aplicada en el método `fit_transform` de la implementación de la librería `scikit-learn`. Tener en cuenta que esta tranformación solo se aplica en la data de entrenamiento, para la data de test aún es necesaria la matriz $V$. por ello en la siguiente implementación aplicaremos la transformación sobre $X$ pero la matriz $\\hat{V}$ debe ser retornada de la misma forma que lo hicimos anteriormente, es decir, hallar la transpuesta de $V^T$ y seleccionar las primeras $K$ columnas ($\\hat{V}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6xpfrT-i4PjG"
   },
   "outputs": [],
   "source": [
    "def PCA_SVD2(X, K):\n",
    "    \"\"\"Reducción de dimensionalidad haciendo uso de SVD sobre un \n",
    "       conjunto de datos X usando K componentes\n",
    "\n",
    "    Args:\n",
    "        X: dataset (N, D), se asume que la data ya está estandarizada\n",
    "        K: número de componentes a considerar en la transformación\n",
    "\n",
    "    Returns:\n",
    "        T: dataset transformado sobre el nuevo eje de coordenadas (N, K)\n",
    "        V_k: matriz columna de los primeros K autovectores (D, K)\n",
    "    \"\"\" \n",
    "    # 1. Descomponer la matriz X haciendo uso de np.linalg.svd\n",
    "    U, S, Vt = \n",
    "    \n",
    "    # 2. Transformar la data haciendo uso de la matriz U (N,N) y el vector\n",
    "    #    S (N)\n",
    "    T = \n",
    "    \n",
    "    # 3. Obtener la matriz 'V' que contiene los autovectores de la matriz de covarianza\n",
    "    V = \n",
    "\n",
    "    # 4. Seleccionar K autovectores\n",
    "    V_k = \n",
    "    \n",
    "    return T, V_k "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hQVJMbDe4RLb"
   },
   "source": [
    "Probemos la implementación mediante visualizaciones para $K=1$ y $K=2$. Las visualizaciones deben de ser las mismas que las mostradas en pasos anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bqR_9eoJ4Sz3"
   },
   "outputs": [],
   "source": [
    "# Obtenemos data reducida 2D\n",
    "T2, _ = PCA_SVD2(X_norm, 2)\n",
    "\n",
    "# Obtenemos data reducida 1D\n",
    "T1, _ = PCA_SVD2(X_norm, 1)\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "pca_2d_plot(T2, \"Principal Components 2D\")\n",
    "plt.subplot(1, 2, 2)\n",
    "pca_1d_plot(T1, \"Principal Components 1D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E9nFmw34P7Sa"
   },
   "source": [
    "## Aplicación sobre el conjunto de datos de iris\n",
    "\n",
    "En esta sección aplicaremos la implementación previamente realizada sobre el conjunto de datos de [iris](https://archive.ics.uci.edu/ml/datasets/Iris). El dataset contiene $150$ muestras de $3$ clases (setosa, versicolor y virginica) y $4$ características (largo del sépalo, ancho del sépalo, largo del pétalo, ancho del pétalo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AeI6iRsvSfMw"
   },
   "outputs": [],
   "source": [
    "### Cargando el dataset\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# conjunto de datos\n",
    "X = iris.data\n",
    "\n",
    "# etiquetas\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gs7RBS4sSttU"
   },
   "source": [
    "### Particionamiento de Datos\n",
    "\n",
    "Como no contamos con un conjunto de datos de prueba (test), particionaremos los datos usando hold-out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZZ3LsjIS4jj"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print('Train size: ', X_train.shape, 'Test size: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aSyj6WkRTZxD"
   },
   "source": [
    "Como tenemos $4$ características, no podemos visualizar directamente la data. Por ello, usaremos PCA para reducir la dimensionalidad y visualizar la data en un menor espacio de características. Por el momento mostraremos algunas filas de los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4n-r791Undg"
   },
   "outputs": [],
   "source": [
    "print(\"Algunos datos de entrenamiento\")\n",
    "print(X_train[0:5])\n",
    "\n",
    "print(\"\\nAlgunas etiquetas del dataset\")\n",
    "print(y_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mkQ7F67OVGC4"
   },
   "source": [
    "### Estandarizar la Data\n",
    "\n",
    "El primer paso antes de realizar PCA es estandarizar la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aP49fjzoVLc8"
   },
   "outputs": [],
   "source": [
    "X_train_norm, X_train_mu, X_train_std = standardize(X_train)\n",
    "\n",
    "print(\"Algunos datos de entrenamiento estandarizados\")\n",
    "print(X_train_norm[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWunX-dHVocn"
   },
   "source": [
    "Usaremos la media y desviación estándar calculadas en la data de entrenamiento para estandarizar la data de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYZl3OlEV2wL"
   },
   "outputs": [],
   "source": [
    "# TODO: Estandarizar la data X_test usando X_train_mu y X_train_std\n",
    "X_test_norm = \n",
    "\n",
    "print(\"Algunos datos de test estandarizados\")\n",
    "print(X_test_norm[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hj17MEk8Wjww"
   },
   "source": [
    "### Reducción de dimensionalidad\n",
    "\n",
    "Usaremos nuestra implementación de PCA para reducir la dimensionalidad de los datos. El primer paso es usar la data de entrenamiento para calcular la matriz de componentes principales, $B$, la cual será usada para proyectar la data de test, de forma similar a lo que hicimos en la estandarización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9vWvH9yoXeeR"
   },
   "outputs": [],
   "source": [
    "# TODO: Usar el método PCA, previamente implementado, para reducir la \n",
    "# dimensionalidad de la data de entrenamiento a 2 dimensiones y obtener \n",
    "# la matriz de componentes principales\n",
    "X_train_pca, B_train = \n",
    "\n",
    "print(\"Dimensión de data de entrenamiento original: \", X_train_norm.shape)\n",
    "print(\"Dimensión de data de entrenamiento transformada: \", X_train_pca.shape)\n",
    "print(\"Dimensión de matriz de componentes: \", B_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VpH_H8YkYxqB"
   },
   "source": [
    "Usaremos la matriz de componentes calculada en la data de entrenamiento para reducir la dimensionalidad de la data de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVuyYB42ZIIN"
   },
   "outputs": [],
   "source": [
    "# TODO: Reducir la dimensionalidad de la data de test estandarizada\n",
    "# (X_test_norm) usando la matriz de componentes B_train.\n",
    "# Recordar que la transformación es una multiplicación de matrices\n",
    "X_test_pca = \n",
    "\n",
    "print(\"Dimensión de data de test original: \", X_test_norm.shape)\n",
    "print(\"Dimensión de data de test transformada: \", X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4boeqrnBaIBX"
   },
   "source": [
    "### Visualización \n",
    "\n",
    "A diferencia del inicio, donde nuestra data original posee $4$ características, podemos usar la data transformada en $2$ dimensiones para visualizarla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0TEDp-SobNaK"
   },
   "outputs": [],
   "source": [
    "def visualizar_iris_dataset(X_train, y_train, X_test, y_test):\n",
    "    plt.subplots(figsize =(15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i in range(0,3):    \n",
    "        plt.scatter(X_train[y_train==i,0], X_train[y_train==i,1])\n",
    "    plt.title('Train Dataset')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.legend(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i in range(0,3):    \n",
    "        plt.scatter(X_test[y_test==i,0], X_test[y_test==i,1])\n",
    "    plt.title('Test Dataset')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.legend(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])\n",
    "    plt.show()    \n",
    "\n",
    "visualizar_iris_dataset(X_train_pca, y_train, X_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtIPKLd8fFo8"
   },
   "source": [
    "### Seleccionando el Número de Componentes Principales\n",
    "\n",
    "Una parte vital del uso de PCA en la práctica es la capacidad de estimar cuántas componentes se necesitan para describir los datos. Esto se puede determinar observando el **ratio o porcentaje de la varianza retenida** en función del número de componentes. Por ello, usaremos los autovalores obtenidos a partir de la matriz de covarianza, ya que los autovalores determinan la varianza retenida por cada componente de manera individual. Por lo tanto, el ratio individual de cada componente está dado por:\n",
    "\n",
    "$$r_i = \\frac{\\lambda_i}{ \\sum_{i=0}^K \\lambda_i} $$\n",
    "\n",
    "donde $\\lambda_i$ es el autovalor correspondiente al $i$-ésimo autovector, $K$ es el número de componentes principales y $r_i$ el ratio de cada componente. Como nuestros autovalores están ordenados de mayor a menor, el primer autovalor es el valor de la mayor varianza retenida por el primer autovector, el segundo autovalor es el valor de la segunda mayor varianza retenida por el segundo autovector, etc. Por lo tanto, el ratio $r_1$ posee la mayor varianza retenida, el ratio $r_2$ posee la segunda mayor varianza retenida, etc. Como el ratio es un valor normalizado de los autovalores, tenemos que:\n",
    "\n",
    "$$ R = \\sum_{i=0}^K r_i = 1 $$\n",
    "\n",
    "es decir, si uso todas las componentes tenemos que: $R = 1$, lo cual me indica que estoy reteniendo toda la varianza de mi data. Si uso menos componentes, el valor del ratio acumulado, $R$, se irá reduciendo. Como mi objetivo es mantener lo más posible la variabilidad de mis datos en un dimensión reducida, entonces vamos a buscar un $R$ con un valor cercano a $1$, por ejemplo: $0.96$, $0.98$, etc. En la práctica se suelen considerar valores $\\geq0.95$, también es bueno hacer visualizaciones de la varianza retenida.\n",
    "\n",
    "\n",
    "Debido a que nuestra implementación del método PCA no retorna la lista de autovalores, haremos una implementación independiente. Sin embargo, es muy recomendable implementar todo lo anterior en una clase ya que es más fácil adicionar componentes como el que tenemos ahora. Como este es un tutorial de implementación, procederemos con métodos. Más adelante usaremos la implementación de la librería por lo que no habrá problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSi1Z9jGn7dV"
   },
   "outputs": [],
   "source": [
    "def retained_variance(X):\n",
    "    \"\"\"Varianza retenida de manera individual y acumulada de las componentes \n",
    "       principales. \n",
    "       \n",
    "       Para calcular la varianza retenida normalizada (ratio):\n",
    "            retained_var(i) = autovalor(i)/ Σ(autovalores)\n",
    "       Para calcular la varianza retenida normalizada acumulada:\n",
    "            cum_retained_var(i) = Σ retained_var(0,i),\n",
    "       es decir, suma acumulada de retained_var del indice 0 al indice actual i\n",
    "\n",
    "    Args:\n",
    "        X: dataset (N, D), se asume que la data ya está estandarizada\n",
    "\n",
    "    Returns:\n",
    "        retained_var: lista con la varianza retenida de cada componente (D)    \n",
    "        cum_retained_var: lista con la varianza retenida acumulada (D)\n",
    "\n",
    "    \"\"\" \n",
    "    # Calcular la matriz de covarianza para las características\n",
    "    cov = \n",
    "\n",
    "    # Calcular los autovalores y autovectores de la matriz de covarianza\n",
    "    (eigvals, eigvecs) =\n",
    "\n",
    "    # Calcular el ratio individual de la varianza retenida usando los autovalores.\n",
    "    # En este método el ratio 'r_i' lleva el nombre de 'retained_var' ya que el \n",
    "    # ratio es la varianza retenida normalizada\n",
    "    retained_var = \n",
    "\n",
    "    # Calcular el varianza retenida acumulada, puede hacer uso del método np.cumsum\n",
    "    cum_retained_var = \n",
    "\n",
    "    return retained_var, cum_retained_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62WNPFD3yeMC"
   },
   "source": [
    "Probamos la implementación anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1WHw0hYIyhNC"
   },
   "outputs": [],
   "source": [
    "retained_var, cum_retained_var = retained_variance(X_train_norm)\n",
    "\n",
    "print(\"Varianza retenida normalizada (ratio)\")\n",
    "print(retained_var)\n",
    "print(\"Varianza retenida acumulada\")\n",
    "print(cum_retained_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0CDK70y0zlFZ"
   },
   "source": [
    "Visualizamos la varianza retenida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AUXGRXeszp7j"
   },
   "outputs": [],
   "source": [
    "def visualize_retained_variance(retained_var):\n",
    "    y_pos = np.arange(len(retained_var))\n",
    "    x_label = [ \"PC%d\"%(pos+1) for pos in y_pos]\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.bar(y_pos, retained_var, align='center')\n",
    "    plt.xticks(y_pos, x_label)\n",
    "    plt.ylabel('Varianza retenida') \n",
    "    plt.title('Varianza retenida por componente principal')\n",
    "    plt.show()\n",
    "\n",
    "visualize_retained_variance(retained_var)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DNH5W9fk3Ywa"
   },
   "source": [
    "Visualizamos la varianza retenida acumulada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KieZA5Rl3cgf"
   },
   "outputs": [],
   "source": [
    "def visualize_cumulative_retained_variance(cum_retained_var):\n",
    "    y_pos = np.arange(len(cum_retained_var))\n",
    "    x_label = [ \"PC%d\"%(pos+1) for pos in y_pos]\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(y_pos, cum_retained_var, marker='o')\n",
    "    plt.xticks(y_pos, x_label)\n",
    "    plt.ylabel('Varianza retenida') \n",
    "    plt.title('Varianza retenida por componente principal')\n",
    "    plt.show()\n",
    "\n",
    "visualize_cumulative_retained_variance(cum_retained_var*100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8rZyTA7_5INz"
   },
   "source": [
    "Como podemos observar en las gráficas anteriores, usando solo la primera componente principal retenemos el $70\\%$ de la varianza de nuestros datos, con las primeras dos componentes retenemos más del $95\\%$. Sin embargo, la tercera y cuarta componente no son del todo útiles en términos de la varianza retenida. Por lo tanto, podemos concluir que usando solamente dos componentes es suficiente para retener la mayor parte de la varianza de la data.\n",
    "\n",
    "Finalmente, implementaremos el método que nos devolverá el número de componentes que retengan una varianza acumulada ingresada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CV4_O5Wa7QE9"
   },
   "outputs": [],
   "source": [
    "def number_of_components(cum_retained_var, desired_var):\n",
    "    \"\"\"Número de componentes que retengan la varianza deseada dada por \n",
    "       'desired_var'. Es necesario usar 'cum_retained_var' para definir el\n",
    "       número de componentes\n",
    "\n",
    "    Args:\n",
    "        cum_retained_var: lista con la varianza retenida acumulada(D)\n",
    "        desired_var: valor entre 0 y 1 indicando la varianza deseada\n",
    "\n",
    "    Returns:\n",
    "        num_components: Número de componentes que retienen la varianza deseada\n",
    "\n",
    "    \"\"\"    \n",
    "    num_components = 0\n",
    "    for i, cum_var in enumerate(cum_retained_var):\n",
    "        if cum_var <= desired_var:\n",
    "           num_components = i + 1\n",
    "        else:\n",
    "           break\n",
    "    return num_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CUEZzb8-9ZLX"
   },
   "source": [
    "Probemos el método implementado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3arkThFp9d4o"
   },
   "outputs": [],
   "source": [
    "percentage = 0.95\n",
    "num_components = number_of_components(cum_retained_var, percentage)\n",
    "\n",
    "print(\"%d components retienen una varianza acumulada <= %lf\" % (num_components, percentage) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ygxsmW2E-X2w"
   },
   "source": [
    "## PCA - Librería scikit-learn\n",
    "\n",
    "Existen librerías que ya tienen una implementación de PCA. Es muy recomendable usar estas librerías por que son probadas por varios desarrolladores y usadas en múltiples productos. En esta oportunidad usaremos la implementación de [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). Para ello importamos la librería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TXV-XUd8ASS2"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jXNZnkpf-HoC"
   },
   "source": [
    "Revisando la documentación podemos encontrar que PCA está implementado mediante Singular Value Decomposition (SVD) y posee diferentes atributos a los que podemos acceder:\n",
    "\n",
    "| Atributo | Descripción | \n",
    "|--|--|\n",
    "|  components_ | matriz de autovectores(eigenvectors)|\n",
    "|  explained_variance_ | varianza retenida por cada componente|\n",
    "|  explained_variance_ratio_ | varianza retenida normalizada por cada componente|\n",
    "|  mean_ | media de la data antes de centrarla |\n",
    "\n",
    "Como podemos observar, todos los atributos los hemos implementado y solamente cambia el nombre en relación a lo realizado por nosotros.\n",
    "\n",
    "Para la primera demostración, replicaremos el proceso realizado anteriormente sobre el dataset de Iris pero con la librería.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "afeVSOiCFDVF"
   },
   "source": [
    "### Aplicación sobre el conjunto de datos de Iris\n",
    "\n",
    "Como ya sabemos acerca de este conjunto de datos, no profundizaremos en algunos pasos de la implementación. Para empezar, cargamos el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TilP35BOF77d"
   },
   "outputs": [],
   "source": [
    "### Cargando el dataset\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# conjunto de datos\n",
    "X = iris.data\n",
    "\n",
    "# etiquetas\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vb0ocel4GOoJ"
   },
   "source": [
    "Ahora particionamos los datos, usamos el mismo valor de random_state usando anteriormente para replicar el particionamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2-4UxYyRGQAp"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print('Train size: ', X_train.shape, 'Test size: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1XfSOnlOGc_c"
   },
   "source": [
    "#### Estandarizar la Data\n",
    "\n",
    "El primer paso antes de realizar PCA es estandarizar la data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7a5kDPxFGf3w"
   },
   "outputs": [],
   "source": [
    "# Importamos la librería\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Definimos el escalador\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Estandarizamos la data de entrenamiento\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "\n",
    "# Estandarizamos la data de test\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "# Imprimimos algunos datos\n",
    "print(\"Algunos datos de entrenamiento estandarizados\")\n",
    "print(X_train_norm[0:5])\n",
    "\n",
    "print(\"\\nAlgunos datos de test estandarizados\")\n",
    "print(X_test_norm[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1F0s-dn4HdNV"
   },
   "source": [
    "#### Reducción de dimensionalidad\n",
    "\n",
    "Usaremos la implementación de la librería scikit-learn para reducir la dimensionalidad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOEdKfVOIEGP"
   },
   "outputs": [],
   "source": [
    "# Definimos PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Reducimos la dimensionalidad de la data de entrenamiento\n",
    "X_train_pca = pca.fit_transform(X_train_norm)\n",
    "\n",
    "# Reducimos la dimensionalidad de la data de test\n",
    "X_test_pca = pca.transform(X_test_norm)\n",
    "\n",
    "# Imprimimos las dimensiones\n",
    "print(\"Dimensión de data de entrenamiento original: \", X_train_norm.shape)\n",
    "print(\"Dimensión de data de entrenamiento transformada: \", X_train_pca.shape)\n",
    "\n",
    "print(\"\\nDimensión de data de entrenamiento original: \", X_test_norm.shape)\n",
    "print(\"Dimensión de data de entrenamiento transformada: \", X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YwNJ61ODIptV"
   },
   "source": [
    "Visualización de la data con dimensionalidad reducida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KHu2uJbhIxyC"
   },
   "outputs": [],
   "source": [
    "visualizar_iris_dataset(X_train_pca, y_train, X_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TUxrDPEHI6TN"
   },
   "source": [
    "#### Seleccionando el Número de Componentes Principales\n",
    "\n",
    "La implementación de scikit-learn nos brinda la varianza retenida normalizada (ratio) accediendo al atributo `explained_variance_ratio_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "waDXyILkK65Y"
   },
   "outputs": [],
   "source": [
    "variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"Varianza retenida normalizada (ratio)\")\n",
    "print(variance_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vdc6YF9gLXF8"
   },
   "source": [
    "Como podemos observar en el resultado anterior, la librería nos retorna la varianza retenida (explicada) normalizada acorde al número de componentes definidos inicialmente. Para replicar los resultados anteriores, tenemos que considerar todas las componentes principales, por tanto es necesario volver a ejecutar PCA sobre la data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lEMc2AghMcA9"
   },
   "outputs": [],
   "source": [
    "# Definimos PCA\n",
    "pca = PCA(n_components=4)\n",
    "\n",
    "# Transformamos la data de entrenamiento sin reducir la dimensionalidad\n",
    "X_train_pca = pca.fit_transform(X_train_norm)\n",
    "\n",
    "# Transformamos la data de test sin reducir la dimensionalidad\n",
    "X_test_pca = pca.transform(X_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILqOj010M6LK"
   },
   "source": [
    "Volvemos a imprimir la varianza retenida por parte de la librería"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-qNlM0kM8s5"
   },
   "outputs": [],
   "source": [
    "variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"Varianza retenida normalizada (ratio)\")\n",
    "print(variance_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uKUOJXVWN2e0"
   },
   "source": [
    "Como podemos observar, ahora si tenemos la varianza retenida de todas las componentes. Procederemos con la visualización respectiva:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yk_Sep3bOKQG"
   },
   "outputs": [],
   "source": [
    "visualize_retained_variance(variance_ratio) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wqeV8z0dOOBl"
   },
   "source": [
    "La varianza retenida normalizada por parte de la librería es la varianza de cada componente, por lo tanto, tenemos que calcular la varianza acumulada y visualizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-eM23wDhOet7"
   },
   "outputs": [],
   "source": [
    "cum_var_ratio = np.cumsum(variance_ratio)\n",
    "visualize_cumulative_retained_variance(cum_var_ratio*100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oChix8blOyEQ"
   },
   "source": [
    "Como podemos observar, hemos obtenido resultados similares a los de nuestra implementación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CTeK9YUHO4f3"
   },
   "source": [
    "## Aplicación sobre un conjunto de datos tabular\n",
    "\n",
    "En esta sección aplicaremos PCA para el conjunto de datos de [vinos](http://archive.ics.uci.edu/ml/datasets/Wine/).  El dataset contiene $178$ muestas de $3$ clases y $13$ características (Alcohol, Malic acid, Ash, Alcalinity of ash, Magnesium, Total phenols, Flavanoids, Nonflavanoid phenols, Proanthocyanins, Color intensity, Hue, OD280/OD315 of diluted wines, Proline) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BYt5SFIaSjzj"
   },
   "outputs": [],
   "source": [
    "# Importamos pandas para cargar los datos\n",
    "import pandas as pd\n",
    "\n",
    "# Descargamos la data\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/'\n",
    "                   'machine-learning-databases/wine/wine.data',\n",
    "                   header=None)\n",
    "\n",
    "# Características\n",
    "data.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n",
    "                   'Alcalinity of ash', 'Magnesium', 'Total phenols',\n",
    "                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "                   'Color intensity', 'Hue',\n",
    "                   'OD280/OD315 of diluted wines', 'Proline']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-bnKXMXATmPU"
   },
   "source": [
    "Verificamos si hay datos NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwL-IvMUTmbT"
   },
   "outputs": [],
   "source": [
    "print(\"NaN de cada columna\")\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8KeqwLH4P2EZ"
   },
   "source": [
    "Mostramos algunas estadísticas de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHwmEt3zT1ss"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DQXIYP8dT-X7"
   },
   "source": [
    "Separamos las características de variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "35giHYEsT09e"
   },
   "outputs": [],
   "source": [
    "y = data['Class label']\n",
    "X = data.drop(columns=['Class label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nwgoeRckP2Ul"
   },
   "source": [
    "### Particionamiento del dataset\n",
    "\n",
    "Como no contamos con un conjunto de datos de prueba (test), particionaremos los datos usando hold-out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xbVz7OwzWSYp"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print('Train size: ', X_train.shape, 'Test size: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCwKtPrMWlfr"
   },
   "source": [
    "### Estandarizamos la data\n",
    "\n",
    "Ahora usaremos un pipeline para realizar todo el proceso de clasificación. Empezamos con un pipeline de preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RCxpojqbXC2w"
   },
   "outputs": [],
   "source": [
    "# Importamos la librería\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Obtenemos nombres de características\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Creamos un pipeline de preprocesamiento\n",
    "preprocessor = Pipeline(steps=[('standardize', StandardScaler())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RlOxZO5FboCz"
   },
   "source": [
    "### Clasificación con todas las características\n",
    "\n",
    "Clasificamos la data con un modelo simple como Regresión Logística\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bYUZreVDcHm6"
   },
   "outputs": [],
   "source": [
    "# Importamos la librería\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Definimos el modelo de clasificación\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Creamos un pipeline para la clasificación\n",
    "baseline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', model)])\n",
    "\n",
    "# Entrenamos el pipeline con la data de entrenamiento\n",
    "baseline.fit(X_train, y_train)\n",
    "\n",
    "# Predecimos valores para la data de test\n",
    "y_pred = baseline.predict(X_test)\n",
    "\n",
    "# Mostramos el accuracy\n",
    "accuracy = (y_test == y_pred).mean()\n",
    "\n",
    "print(\"Baseline accuracy: %.3lf\" % accuracy )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "As6VTMOnXgS3"
   },
   "source": [
    "### Selección de número de componentes\n",
    "\n",
    "Primero hacemos el análisis para poder determinar el número de características a usar. Para ello, realizamores las gráficas de la varianza retenida por cada componente. Como solo tenemos $13$ características, podemos usarlas todas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IXWNG8PfYnXq"
   },
   "outputs": [],
   "source": [
    "# Definimos PCA\n",
    "pca = PCA(n_components=13)\n",
    "\n",
    "# Definimos un pipeline que combina tanto el preprocesamiento como pca\n",
    "feature_extractor = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                    ('pca', pca)])\n",
    "\n",
    "# Transformamos la data de entrenamiento sin reducir la dimensionalidad\n",
    "X_train_pca = feature_extractor.fit_transform(X_train)\n",
    "\n",
    "# Transformamos la data de test sin reducir la dimensionalidad\n",
    "X_test_pca = feature_extractor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_wUEQJkKZEcX"
   },
   "source": [
    "Obtenemos la varianza retenida normalizada (ratio) accediendo al atributo `explained_variance_ratio_` y calculamos la varianza cumulada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0IFfAvfZPxM"
   },
   "outputs": [],
   "source": [
    "# Obtenemos la varianza retenida normalizada (ratio) para cada componente\n",
    "variance_ratio = feature_extractor.named_steps['pca'].explained_variance_ratio_\n",
    "\n",
    "# Calculamos la varianza acumulada\n",
    "cum_var_ratio = np.cumsum(variance_ratio)\n",
    "\n",
    "print(\"Varianza retenida normalizada (ratio)\")\n",
    "print(variance_ratio)\n",
    "print(\"Varianza retenida acumulada\")\n",
    "print(cum_var_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9yeS1cAa6Z4"
   },
   "source": [
    "Ahora realizaremos las visualizaciónes correspondientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VX3r6VlLbCw_"
   },
   "outputs": [],
   "source": [
    "visualize_retained_variance(variance_ratio) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U1oT8mcNbFkV"
   },
   "outputs": [],
   "source": [
    "visualize_cumulative_retained_variance(cum_var_ratio*100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZRu0foAbKGR"
   },
   "source": [
    "Podemos observar que con $10$ características obtenemos más del $95\\%$ de la varianza retenida, por tanto, ese número es una buena opción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZKyIofkrdpF0"
   },
   "source": [
    "### Clasificación con menos características\n",
    "\n",
    "A continuación, realizaremos la clasificación con un menor número de características, obtenidos a partir de PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bwYtx5oSduRf"
   },
   "outputs": [],
   "source": [
    "# Definimos el modelo de clasificación\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Definimos PCA\n",
    "pca = PCA(n_components=10)\n",
    "# ejemplo ingresando porcentage de varianza retenida deseada\n",
    "# pca = PCA(n_components=0.9)\n",
    "\n",
    "# Creamos un pipeline para la clasificación\n",
    "model_pca = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                            ('pca', pca),\n",
    "                            ('classifier', model)])\n",
    "\n",
    "# Entrenamos el pipeline con la data de entrenamiento\n",
    "model_pca.fit(X_train, y_train)\n",
    "\n",
    "# Predecimos valores para la data de test\n",
    "y_pred = model_pca.predict(X_test)\n",
    "\n",
    "# Mostramos el accuracy\n",
    "accuracy = (y_test == y_pred).mean()\n",
    "\n",
    "print(\"PCA Model accuracy: %.3lf\" % accuracy )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3JrhOsDDe3Ya"
   },
   "source": [
    "Como podemos observar, hemos obtenido un resultado similar usando una menor dimensionalidad. También podemos hacer use del porcentaje de varianza retenida deseado en lugar del número de compontenes. Para ello, se coloca un valor entre 0 y 1 como n_components y  si deseamos saber el número de componentes hallado lo podemos hacer de la siguiente manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWFEPSAOfr0f"
   },
   "outputs": [],
   "source": [
    "model_pca.named_steps['pca'].n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xLq_zQV8CuEK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Taller_PCA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
